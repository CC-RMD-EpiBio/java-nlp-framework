// =================================================
/**
 * LorettaAnnotator looks up text via the ciitizen terminology service's nlp2
 * mechanism.
 * 
 * 
 * 
 * This is built directly upon UIMA's implementation of
 * an annotator - JCasAnnotator_ImplBase.  
 * 
 * This class creates annotations that get defined 
 * from a uima type descriptor found in the
 * 06_type.descriptor/src/main/resources/com/ciitizen/framework/LorettaModel.xml
 *
 * This class refers to annotations that got defined
 * in the framework-type.descriptors project/repo
 *
 * @author  Guy Divita 
 * @created Jun 1, 2018
 *
 * 
 * 

 */
// ================================================
package gov.nih.cc.rmd.nlp.framework.annotator.loretta;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Set;

import org.apache.uima.UimaContext;
import org.apache.uima.analysis_component.JCasAnnotator_ImplBase;
import org.apache.uima.analysis_engine.AnalysisEngineProcessException;
import org.apache.uima.cas.Type;
import org.apache.uima.jcas.JCas;
import org.apache.uima.jcas.cas.StringArray;
import org.apache.uima.jcas.cas.TOP_Type;
import org.apache.uima.jcas.tcas.Annotation;
import org.apache.uima.resource.ResourceInitializationException;

import com.ciitizen.Medication;
import com.ciitizen.framework.model.DateTime;
import com.ciitizen.framework.model.Shape;

import gov.nih.cc.rmd.nlp.framework.utils.terminologies.TermLookupInterface;
import gov.nih.cc.rmd.nlp.framework.utils.terminologies.lookup.LRAGRRow;
import gov.nih.cc.rmd.nlp.framework.utils.terminologies.lookup.TermLookupLocalTermsImpl;
import gov.nih.cc.rmd.nlp.framework.utils.terminologies.utilities.TermUtils;

import gov.va.chir.model.ClinicalStatement;
import gov.va.chir.model.ContentHeading;
import gov.va.chir.model.DependentContent;
import gov.va.chir.model.DocumentHeader;
import gov.va.chir.model.LexicalElement;
import gov.va.chir.model.Sentence;
import gov.va.chir.model.Token;
import gov.va.chir.model.Utterance;
import gov.va.chir.model.VAnnotation;
import gov.va.chir.model.WordToken;
import gov.va.vinci.model.AssertionEvidence;
import gov.va.vinci.model.Concept;
import gov.va.vinci.model.NegationEvidence;
import gov.nih.cc.rmd.nlp.framework.utils.GLog;
import gov.nih.cc.rmd.nlp.framework.utils.ProfilePerformanceMeter;
import gov.nih.cc.rmd.nlp.framework.utils.U;
import gov.nih.cc.rmd.nlp.framework.utils.framework.uima.VUIMAUtil;
import gov.nih.cc.rmd.nlp.framework.utils.uima.UIMAUtil;



public class LorettaAnnotator extends JCasAnnotator_ImplBase  {
 

  // -----------------------------------------
  /**
   * process retrieves lines of the document, labels those that are questions
   * as QuestionAndAnswer elements.
   * 
   * 
   */
  // -----------------------------------------
  public void process(JCas pJCas) throws AnalysisEngineProcessException {
   
    try {
    this.performanceMeter.startCounter();
    
    if ( this.processMe ) {
   
      unmarkAllAnnotations(pJCas);
  
     // [tbd] break this to pieces 
  // Utterances include sentence slotValue, content headings, dependent content ....
  // if you use utterance, the same span could get termized multiple times because it
  // is within the bounds of a slotValue, contentHeading and sentence all at the same time.
      
    List<Annotation> sentences = UIMAUtil.getAnnotations(pJCas, Sentence.typeIndexID, true);
  
  if (sentences != null && !sentences.isEmpty()) {

   
    UIMAUtil.sortByOffset( sentences );
    sentences = UIMAUtil.uniqueAnnotations(sentences);
    // ------------------------------------------------------
    // Walk through the utterances, looking for those that
    // are contentHeaders, and not slotValue's.
    // ------------------------------------------------------
    for (Annotation aSentence : sentences ) {
       String sent = aSentence.getCoveredText();
       termTokenizeSentence(pJCas, (Sentence) aSentence);
    }
       
  
  } // end if there are any sentences

  // ---------------------------------------------
  // Process the content headings of those slot:values and questions that are asserted
  // ---------------------------------------------
  /*
  List<Annotation> contentHeadings = UIMAUtil.getAnnotations(pJCas, ContentHeading.typeIndexID);
  if ( contentHeadings != null && contentHeadings.isEmpty() ) {
    for ( Annotation aHeading : contentHeadings )
       termTokenizeSentence(pJCas, (ContentHeading) aHeading );
     
  }
  */
  
  // ---------------------------------------------
  // Process the dependent content of  slot:values 
  // ---------------------------------------------
  List<Annotation> dependentContents = UIMAUtil.getAnnotations(pJCas, DependentContent.typeIndexID);
 
  if ( dependentContents != null && !dependentContents.isEmpty() ) 
    for ( Annotation dependentContent : dependentContents) 
      termTokenizeSentence(pJCas,  (Utterance) dependentContent );
     
    
  
  
  // ----------------------------------
  // Final repair - look for terms that contain hyphens, that are not from any lexicon
  //   break them into their parts and add terms for each constituent.
  // repairHyphenatedTerms(pJCas);
  
    }
  
   
    
    this.performanceMeter.stopCounter( );
    
    } catch (Exception e) {
      e.printStackTrace();
      GLog.println(GLog.ERROR_LEVEL,this.getClass(), "process", "Issue with " + this.getClass().getName() + " " + e.toString());
   //   throw new AnalysisEngineProcessException();
    }
  } // end Method process() ----------------
   
 



  // =================================================
  /**
   * unmarkAllAnnotations unmarks a parashiable marker
   * to know if I've already processed the structure
   * or not.
   * 
   * @param pJCas
  */
  // =================================================
  private void unmarkAllAnnotations(JCas pJCas) {
    
    List<Annotation> allAnnotations = UIMAUtil.getAnnotations(pJCas, VAnnotation.typeIndexID );
    if ( allAnnotations != null && !allAnnotations.isEmpty()) {
      for ( Annotation annotation : allAnnotations ) {
        if ( ((VAnnotation)annotation).getProcessMe() )
          ((VAnnotation)annotation).setMarked( false );
        else 
          ((VAnnotation)annotation).setMarked( true );
      }
      
    }
    
  } // end Method unmarkAllAnnotations() ------------



  // -----------------------------------------
  /**
   * termTokenizeSentence creates terms from the
   * tokens in this sentence.
   * 
   * @param pJCas
   * @param pSentence
   * @return 
   * @throws Exception 
   */
  // -----------------------------------------
  public List<Annotation> termTokenizeSentence(JCas pJCas,  Utterance pSentence) throws Exception {
    
 
    List<Annotation> terms = null;
    // only process terms in sections that are useful 
    if (this.isThisInASectionToProcess( pJCas, pSentence) ) {
   
    if ( !pSentence.getMarked() ) {
   
      List<Annotation> tokens = UIMAUtil.getAnnotationsBySpan(pJCas, WordToken.typeIndexID, pSentence.getBegin(), pSentence.getEnd());

      tokens = UIMAUtil.uniqueAnnotations(tokens);
      UIMAUtil.sortByOffset(tokens);
      
      String[] terminologies = getTerminologiesFromSection( pJCas, pSentence );
      String            tree = getTreeFromSection( pJCas, pSentence);
      
      if ( tokens != null ) {
        
        if ( !areAnyTokensAlreadyMapped( pJCas, tokens) ) 
          terms =  termTokenizeSentence( pJCas, terminologies, tree, tokens);
        else {
          List<Annotation>someTokens = getNextUnmappedTokens( tokens );
          while ( someTokens != null  ) {
            terms = termTokenizeSentence( pJCas, terminologies, tree, someTokens );
            someTokens = getNextUnmappedTokens( tokens );
          }
        }
    } // end if there are any tokens in this utterance
    
      pSentence.setMarked(true);
    } // end if sentence has been processed
    
    }
    return terms;
    
  }  // end Method termTokenizeSentence() ----------------
    
    // =================================================
    /**
     * isThisInASectionToProcess does a lookup to find
     * annotation type (say medication, diagnosis ...) 
     * and returns the kinds of sections these annotations
     * are in.  
     * This method returns true if the filterBySection (--filterBySection=) 
     * parameter that is passed in is "false".  By default
     * this is "true".
     * 
     * This method looks to see if the document header has
     * a section type filled out - if so, that's the overriding
     * section to queue from. 
     * 
     * This method does a lookup into the resources/com/framework/sections/conentSectionNames hash
     * to determine if this section type equates to this annotator type.
     * 
     * 
     * @since 2018.08.9
     * 
     * @param pJCas
     * @param pSentence
     * @return boolean
     */
  // =================================================
   boolean isThisInASectionToProcess(JCas pJCas, Utterance pSentence) {
    
     boolean returnVal = true;
     
      DocumentHeader documentHeader = VUIMAUtil.getDocumentHeader(pJCas);
      String sectionName = null;
      
      
      if ( documentHeader.getSectionType() != null )
        sectionName = documentHeader.getSectionType();
     
     if ( this.filterBySection ) {
     
       if ( sectionName == null )
         sectionName = VUIMAUtil.getSectionName(pSentence);
     pSentence.setSectionName( sectionName);
     String annotationType = this.annotationType;   // <----- replace 
     
    
     returnVal = this.sectionInfo.isValidSectionFor( sectionName, annotationType);
    // String msg = "section type = " + sectionName + "|" + annotationType + "|" + returnVal;
    
     }
     
    return returnVal;
   }  // end Method isThisInASectionToProcess() 
  


  // =================================================
  /**
   * areAnyTokensAlreadyMapped checks to see if any of
   * these tokens have been consumed in any other clinicalStatment/concept/date/email/address/url thing 
   *   I should make a super class for email/address/url  <- there maybe "shape" or "group"
   *   There are two ways to do this - query to see if there are any overlapping concepts 
   *   is the most guarenteed way to do this, but this is expensive. 
   *   
   *   Another way is to check the token's parent to see if it's filled out - that will tell if
   *   it's covered by a lexicical element.  Check the lexical element's parent then to see if
   *   it points to a concept.  I wasn't completely 100% that happens.
   *   
   *   
   * @param pTokens
   * @return boolean
  */
  // =================================================
 private final boolean areAnyTokensAlreadyMapped(JCas pJCas, List<Annotation> pTokens) {
  
   boolean returnVal = false;
   
   if ( pTokens != null && !pTokens.isEmpty() ) {
      for ( Annotation token : pTokens ) {
        if ( isCovered(pJCas, token) ) {
            returnVal = true;
            ((Token)token).setMarked(true);
            
        }
      }
   }
     
   return returnVal;
  } // end Method areAnyTokensAlreadyMapped() --------

 //=================================================
 /**
  * isCovered returns true if ths annotation is covered
  * by a mapped concept or something that should be ignored
  * by the mapper
  * 
  * @param pJCas
  * @param pToken
  * @return boolean 
  */
// =================================================
 private final boolean isCovered(JCas pJCas, Annotation pToken) {
   boolean returnVal = false;
   
   Type aType = pJCas.getCasType(AssertionEvidence.type); 
   returnVal = isCovered( pJCas,  AssertionEvidence.typeIndexID, aType, pToken);
  
   
   if ( !returnVal ) {
  
      aType = pJCas.getCasType(DateTime.type); 
      returnVal = isCovered( pJCas,  DateTime.typeIndexID, aType, pToken);
  
   
   if ( !returnVal ) {
     aType = pJCas.getCasType(Shape.type);
     returnVal = isCovered( pJCas,  Concept.typeIndexID, aType, pToken);
     
     if ( !returnVal ) {
       aType = pJCas.getCasType(ClinicalStatement.type);
       returnVal = isCovered( pJCas,  ClinicalStatement.typeIndexID, aType, pToken);
    
       if ( !returnVal ) {
         aType = pJCas.getCasType(Concept.type);
         returnVal = isCovered( pJCas,  Concept.typeIndexID, aType, pToken);
       }
     }
   }
   }
       
   return returnVal;
 }  // end Method isCovered() -----------------------

  // =================================================
  /**
   * isCovered returns true if ths annotation is covered
   * by a mapped concept or something that should be ignored
   * by the mapper
   * 
   * @param pJCas
   * @param pToken
   * @return boolean 
  */
  // =================================================
  private final boolean isCovered(JCas pJCas, int pTypeIndexID, Type pType, Annotation pToken) {
    boolean returnVal = false;
     
   
    List<Annotation> coveredEntities = UIMAUtil.fuzzyFindAnnotationsBySpan(pJCas, pType, pTypeIndexID, pToken.getBegin(), pToken.getEnd(), true );
    
    if ( coveredEntities != null && !coveredEntities.isEmpty() ) {
      
      // check to see if this is a shape and if so, if it is not a unit of measure - something we want to ignore currently for medications
       if ( coveredEntities.get(0).getClass().getSimpleName().contains("UnitOfMeasure")  || 
            coveredEntities.get(0).getClass().getSimpleName().contains("Number")         ||
            coveredEntities.get(0).getClass().getSimpleName().contains("Gold")    )
          returnVal = false;
        else 
          returnVal = true;
    }
    
    
     
   
    return returnVal;
  } // end Method isCovered() -------------------------



  // =================================================
  /**
   * getNextUnmappedTokens returns the next set of 
   *  unmarked (unprocessed) tokens.  
   *  
   *  Tokens are marked when they go through the isCoverd method.
   *  Tokens are marked when they go through the
   *  lookup method.
   * 
   * @param tokens
   * @return List of Annotations
  */
  // =================================================
 private List<Annotation> getNextUnmappedTokens(List<Annotation> pTokens) {
  
   List<Annotation> returnVal = null;
   List<Annotation> unMarkedTokens = new ArrayList<Annotation>();
   if ( pTokens != null && !pTokens.isEmpty() ) {
     int i = 0;
     // Iterate across/through tokens that have already been processed
     for ( ; i < pTokens.size(); i++  ) 
       if (! ((Token) pTokens.get(i)).getMarked() ) 
           break;
     // Now itterate through and accumulate the contiguous batch of tokens that have not been processed
     for (; i < pTokens.size(); i++ )
       if ( ((Token) pTokens.get(i)).getMarked() ) 
         break;
       else
         unMarkedTokens.add( pTokens.get(i)) ;
      }
   
      if ( unMarkedTokens.size() > 0)
        returnVal = unMarkedTokens;
      
      return returnVal;
  
    
  } // end Method getNextUnmappedTokens() -----------



  // =================================================
  /**
   * getTerminologiesFromSection returns a terminology
   * based on what section this annotation is in
   * 
   * i.e., look in rxnorm if you are in a medications section
   *       look in loinc if you are in vitals, labs ... section
   *       otherwise look in snomedct_us
   * 
   * @param pJCas
   * @param pSentence
   * @return String[]
  */
  // =================================================
  public String[] getTerminologiesFromSection(JCas pJCas, Utterance pSentence) {
    
    String[] terminologies =  this.terminologies; 
    
    return terminologies;
  } // end Method getTerminologyFromSection()----------

  // =================================================
  /**
   * getTree FromSection returns a tree
   * based on what section this annotation is in
   * 
   * i.e., look in the procedures tree from the snomed 
   * 
   * @param pJCas
   * @param pSentence
   * @return String[]
  */
  // =================================================
  public String getTreeFromSection(JCas pJCas, Utterance pSentence) {
    
    String tree =  this.tree; 
    
    return tree;
  } // end Method getTerminologyFromSection()----------




  // -----------------------------------------
  /**
   * lookup retrieves the terms from a sentence
   * 
   * @param pJCas
   * @param pTerminologies 
   * @param tree
   * @param pSentenceTokens
   * @return List<LexicalElement>
   * 
   * @throws Exception 
   */
  // -----------------------------------------
  private List<Annotation> termTokenizeSentence(JCas pJCas, String[] pTerminologies, String tree, List<Annotation> pSentenceTokens ) throws Exception {
   
   List<Annotation> terms = null;
    if ( pSentenceTokens != null  && !pSentenceTokens.isEmpty() ) 
      terms = lookup(pJCas, pTerminologies, tree, pSentenceTokens );
    return terms ;
          
  } // end Method termTokenizeSentence() ---------------------------

  // =================================================
  /**
   * lookup looks the non whitespace tokens up in a lexical lookup service
   *  It marks tokens as processed so that they don't get accidently absorbed
   *  again later on.
   *  It creates terms from terms found from these tokens
   * 
   * @param pJCas
   * @param pTerminologies
   * @param pTree
   * @param pSentenceTokens
   * @return List<LexicalElement>
   * @throws Exception 
  */
  // =================================================
 private final List<Annotation> lookup( JCas pJCas, String[] pTerminologies, String pTree, List<Annotation> pTokens) throws Exception {
    
   List<Annotation> terms = new ArrayList<Annotation>();
   
   
  
   if ( pTokens != null && !pTokens.isEmpty()) {
   List<Annotation> tokens = makeTokens( pTokens, 0, pTokens.size()); // <---- strips sentence ending punctuation
  
   if ( tokens != null && !tokens.isEmpty()) {
     // Call the terminology service with the tokens 
     // Retrieve the terms found
     String key = makeKeyFrom(pJCas, tokens); 
     if ( key != null && !key.equals("null")) {
       List<TempToken>tempTokens = createTempTokens( key);
       alignTokens( tempTokens, tokens);
  
       List<LRAGRRow> results = getSimple(pTerminologies, pTree, key);
     
     // ------------------------------
     // what comes back are lragr rows with offsets on them
     // make a term for each set of offsets
     //   within the term, make row entries for each row that covers that offset
       if ( results != null && !results.isEmpty()) { 
         HashMap<String, List<LRAGRRow> > offsetPairRows = divideRowsByOffsets( results);
     
         if ( offsetPairRows != null && !offsetPairRows.isEmpty()) {
     
         Set<String> keyz = offsetPairRows.keySet();
         String[] keys = keyz.toArray(new String[keyz.size()]);
         Arrays.sort( keys);
         
         for ( String aKey : keys ) {
            if ( offsetPairRows.get( aKey ) != null && !offsetPairRows.get( aKey ).isEmpty() ) {
             List<LRAGRRow> rows = offsetPairRows.get(aKey);
             if ( rows != null  && !rows.isEmpty()) {
               LRAGRRow aRow = rows.get(0);
               if ( aRow != null && aRow.getCuis() != null && !aRow.getCuis().equals("unknown")) {
                 TempTerm aTempTerm = makeTmpTermFromKey( pJCas, aKey, rows);
                 attachTmpTokens(aTempTerm, tempTokens);
              //   rows = this.wsdAnnotator.disambiguate( pJCas, aTempTerm, rows );
                 
                 if ( rows != null && !rows.isEmpty()) {
                   Annotation newTerm = makeTermFromLRAGRRows( pJCas, aTempTerm,  rows );
                   if (newTerm != null )
                     terms.add( (Annotation) newTerm);
                   
                 }
               }
             }
             }
           
               } 
         }
         }
     }
     }
     markTokens( pTokens);
   }
   
   if ( terms.isEmpty() ) terms = null;
   return terms ;
    
  } // End lookup() ---------------------------------

 
  // =================================================
  /**
   * markTokens marks that tokens as been seen
   * 
   * @param pTokens
  */
  // =================================================
  private final void markTokens(List<Annotation> pTokens) {
    
    for ( Annotation aToken : pTokens)
      ((Token) aToken).setMarked(true);
    
  } // end Method markTokens() -----------------------



  // =================================================
  /**
   * alignTokens attaches the real token to the 
   * temp token - there should be a one-to-one-correspondence
   * between the two
   * 
   * @param pTempTokens
   * @param pTokens
  */
  // =================================================
 private void alignTokens(List<TempToken> pTempTokens, List<Annotation> pTokens) {
   
   try {
   for ( int i = 0; i < pTempTokens.size(); i++ )  {
   
     pTempTokens.get(i).setRealToken( pTokens.get(i));
   }
   } catch (Exception e) {
     
     /* ----------------------------------
      *  If we get an exception here there is 
      *  a mis-alignment between the token annotations
      *  and the normalized string which has been
      *  stripped of things that would not
      *  be in an index.  These are punctuation
      *  differences.  
      *
     e.printStackTrace();
     String msg = "Issue with aligning the tokens " + e.toString() ;
     GLog.println(GLog.ERROR_LEVEL, this.getClass(), "alignTokens", msg);
     for ( int i = 0; i < pTempTokens.size(); i++ )  {
      String msg2 = null;
      try {
        msg2 = pTempTokens.get(i).getCoveredText() + "|" + pTokens.get(i).getCoveredText() ;
        GLog.println(GLog.ERROR_LEVEL, this.getClass(), "alignTokens", msg2);
      } catch (Exception e2) {};
     }
     // throw e;
      * 
      */
   }
   
   
  } // end Method alignTokens) -----------------------



  // =================================================
  /**
   * attachTmpTokens 
   * 
   * @param aTempTerm
   * @param tempTokens
  */
  // =================================================
 private final void attachTmpTokens(TempTerm aTempTerm, List<TempToken> tempTokens) {
   
  for ( TempToken aToken : tempTokens ) {
    
    if (   aToken.getBeginOffset() >= aTempTerm.getBeginOffset() &&
           aToken.getEndOffset()   <= aTempTerm.getEndOffset() ) 
      aTempTerm.addToken( aToken);
    else if ( aToken.getBeginOffset() > aTempTerm.getBeginOffset()) 
      break;
  } // end loop thru temp tokens
  
  } // end Method attachTmpTokens() ------------------



  // =================================================
  /**
   * makeTmpTermFromKey 
   * @param pJCas 
   * 
   * @param aKey
   * @param pRows
   * @return
  */
  // =================================================
  private TempTerm makeTmpTermFromKey(JCas pJCas, String aKey, List<LRAGRRow> pRows) {
 
    TempTerm aTerm = null;
    String fullText = pJCas.getDocumentText();
    if ( fullText != null ) {
      String cols[] = U.split(aKey);
      int beginOffset = Integer.parseInt(cols[0]);
      int endOffset = Integer.parseInt(  cols[1]);
      String someText = fullText.substring(beginOffset,  endOffset + 1);
      
      
    String pText = U.normalize(someText );
    int pBeginOffset = pRows.get(0).getBeginOffsetInQuery();
    int pEndOffset = pRows.get(0).getEndOffsetInQuery();
    aTerm = new TempTerm( pText, pBeginOffset, pEndOffset);
    
    }
    return aTerm;
  } // end Method makeTmpTermFromKey() ---------------



  // =================================================
  /**
   * createTempTokens returns a set of tokens tokenized
   * the same way the lookup happens
   * 
   * @param key
   * @return
   * @throws Exception 
  */
  // =================================================
 private final List<TempToken> createTempTokens(String pKey) throws Exception {
    
   List<String> someTokens = this.tokenize.tokenize(pKey);
   
   int currentPtr = -1;
   ArrayList<TempToken> tempTokens = new ArrayList<TempToken>();
   for ( String aTokenString : someTokens ) {
     int startingOffset = pKey.indexOf(aTokenString, currentPtr );
     
     if ( startingOffset != -1) {
       TempToken aTempToken = new TempToken(aTokenString, startingOffset, startingOffset + aTokenString.length() );
       tempTokens.add( aTempToken);
       currentPtr = startingOffset + 1;
     }
     
   }
   
   return tempTokens;
   
  } // end Method createTempTokens() ----------------



  // =================================================
  /**
   * makeTermFromLRAGRRow
   *  createTerm
   * 
   * @param pRows
   * @return List<Annotation>
  */
  // =================================================
  protected Annotation makeTermFromLRAGRRows(JCas pJCas, TempTerm aTempTerm, List<LRAGRRow> pRows ) {
    
    LexicalElement statement = new LexicalElement(pJCas);
   
   List<Annotation> tokens = aTempTerm.getRealTokens();
    if ( tokens == null || tokens.isEmpty()) {
      GLog.println(GLog.ERROR_LEVEL, "Issue with term with no aligned tokens " );
      return null;
    }
    int beginOffset = tokens.get(0).getBegin();
    int endOffset   = tokens.get(tokens.size() -1).getEnd();
   
    // convert pRows into an array of String
    List<String> resultRows = new ArrayList<String>(pRows.size());

    StringArray lexMatches = null;
    
    for ( LRAGRRow row: pRows ) {
      String cuis = row.getCuis();
      if ( cuis != null && !cuis.equals("unknown"))
        resultRows.add(row.toLRAGRString() );
    }
    if ( resultRows != null && !resultRows.isEmpty() )
     lexMatches = UIMAUtil.list2StringArray(pJCas, resultRows);
    
    
    statement.setBegin( beginOffset);
    statement.setEnd(    endOffset );
    statement.setId("Loretta_" + annotationCounter );
    
    if ( isInContentHeading( pJCas, (Annotation)statement ))  return null;
    if ( isInOtherConcept(   pJCas, (Annotation)statement ))  return null;
   
    statement.setLexMatches( lexMatches );
    statement.addToIndexes();
    
    String sectionName = VUIMAUtil.deriveSectionName(statement);
    statement.setSectionName( sectionName );
   
    return statement;
    
    
  } // end Method makeTermFromLRAGRRow() -------------



  // =================================================
  /**
   * divideRowsByOffsets 
   * 
   * @param pRows
   * @return HashMap<String, List<LRAGRRow> >  offsets|Sets of rows
  */
  // =================================================
 private final HashMap<String, List<LRAGRRow>> divideRowsByOffsets(List<LRAGRRow> pRows) {
 
   HashMap<String, List<LRAGRRow> > returnVal = new HashMap<String, List<LRAGRRow> >(pRows.size());
   
   for ( LRAGRRow row: pRows ) {
     String key = row.getRowOffsetKey();
     List<LRAGRRow> slot = returnVal.get( key);
     if ( slot == null) {
       slot = new ArrayList<LRAGRRow>();
       returnVal.put(key , slot);
     }
     slot.add( row );
   } // end loop thru rows
   
   return returnVal;
 } // end Method divideRowsByOffsets() --------------



  // =================================================
  /**
   * makeKeyFrom returns the string that was covered by
   * the set of tokens.  The key will have whitespace normalized.
   * Case is preserved.
   * 
   * @param pTokens
   * @return String
   * @throws Exception 
  */
  // =================================================
  private String makeKeyFrom(JCas pJCas, List<Annotation> pTokens) throws Exception {
    String returnVal = null;
    
    StringBuffer buff = new StringBuffer();
    for ( Annotation token : pTokens ) {
      String tokenText = token.getCoveredText();
       tokenText = U.normalize( tokenText.trim() );
       buff.append( tokenText );
       buff.append( " ");
    }
    returnVal = buff.toString().trim();
    GLog.println(GLog.DEBUG_LEVEL, this.getClass(), "makeKeyFrom", "key = |" + returnVal + "|");
   
   
    return returnVal;
  } // end Method makeKeyFrom() ----------------------

  
  
  
  // =================================================
  /**
   * get 
   * 
   * @param pKey
   * @param pTree  
   * @return List<LragrRow>  rows from the table
  */
  // =================================================
    protected List<LRAGRRow> getSimple(String[] pTerminologies, String pTree, String pKey) {
    
      
      String key = null;
      List<LRAGRRow> returnVal = null;
      List<LRAGRRow> results = null;
      
      try {
      
       // key = this.tokenize.normalizeNotReverse( pKey );
        key = pKey;
      
        for ( String terminology: pTerminologies ) {
          
     
     
         
          results = this.terminologyService.getNotSimple( terminology, pTree, key );
          
          if ( results != null && !results.isEmpty()) {
            if ( returnVal == null ) returnVal = new ArrayList<LRAGRRow>();
            returnVal.addAll( results);
          }
        }
        
        // ------------------------
        // There seems to be a bug from the service, where one of the rows might be the whole
        // key, when there are also additional pieces, so if there are more than one row
        // and one of the rows is the whole key, then filter this one out
        // -----------------------
        if ( returnVal != null && !returnVal.isEmpty() ) {
          if ( areThereMoreThanOneSubTerms( returnVal ) ) 
            returnVal = filterOutFullLengthRows( returnVal, key);
        }
        
      } catch (Exception e) {
        e.printStackTrace();
        String msg = "Issue with the term service with term " + key + " " + e.toString();
        GLog.println(GLog.ERROR_LEVEL, msg);
      }
        
    return returnVal;
  } // end Method get() ------------------------------



    // =================================================
    /**
     * getDefaultWithFilter
     * 
     * @param pTerminologies
     * @param pAttributes
     * @param pKey
     * @deprecated    - this method has been hollowed out.  Write an alternative.
     * @return List<LragrRow>  rows from the table
    */
    // =================================================
      protected List<LRAGRRow> getDefaultWithFilter(String[] pTerminologies, String pAttributes, String pKey) {
      
        List<LRAGRRow> returnVal = null;
        /*
        String key = null;
       
        List<LRAGRRow> results = null;
        
        try {
        
         // key = this.tokenize.normalizeNotReverse( pKey );
          key = pKey;
        
          for ( String terminology: pTerminologies ) {
           
            results = this.terminologyService.getDefaultWithFilter( terminology, pAttributes, key );
            
            if ( results != null ) {
              if ( returnVal == null ) returnVal = new ArrayList<LRAGRRow>();
              returnVal.addAll( results);
            }
          }
          
          // ------------------------
          // There seems to be a bug from the service, where one of the rows might be the whole
          // key, when there are also additional pieces, so if there are more than one row
          // and one of the rows is the whole key, then filter this one out
          // -----------------------
          if ( returnVal != null && !returnVal.isEmpty() ) {
            if ( areThereMoreThanOneSubTerms( returnVal ) ) 
              returnVal = filterOutFullLengthRows( returnVal, key);
          }
          
        } catch (Exception e) {
          e.printStackTrace();
          String msg = "Issue with the term service with term " + key + " " + e.toString();
          GLog.println(GLog.ERROR_LEVEL, msg);
        }
          
          */
      return returnVal;
    } // end Method get() ------------------------------

    
  
 // =================================================
  /**
   * filterOutFullLengthRows 
   * 
   * @param returnVal
  */
  // =================================================
  private final List<LRAGRRow> filterOutFullLengthRows(List<LRAGRRow> pLragrRows, String pKey) {
    
 
    ArrayList<LRAGRRow> returnVal = null;
    
    if ( pLragrRows != null && !pLragrRows.isEmpty()) {
      
      returnVal = new ArrayList<LRAGRRow>( pLragrRows.size() );
      for ( LRAGRRow row : pLragrRows ) {
      
        if (( row.getBeginOffsetInQuery() == 0 ) &&
            (row.getEndOffsetInQuery() == pKey.length() )) 
        ;
         else {
          returnVal.add( row );
        }
      }
    }
   
     return returnVal;
  } // end Method filterOutFullLengthRows() ----------



  // =================================================
  /**
   * areThereMoreThanOneSubTerms [TBD] summary
   * 
   * @param pLragrRows
   * @return boolean
  */
  // =================================================
  private final boolean areThereMoreThanOneSubTerms(List<LRAGRRow> pLragrRows) {
   
    boolean returnVal = false;
    int anOffsetBegin = 0;
    int previousOffset = 0;
    for ( LRAGRRow row : pLragrRows ) {
      
      anOffsetBegin = row.getBeginOffsetInQuery();
     
      
      if (anOffsetBegin != previousOffset && anOffsetBegin != 0) {
        returnVal = true;
        break;
      }
      previousOffset = anOffsetBegin;
    }
    
    return returnVal;
  } // end Method areThereMoreThanOneSubTerms() ----



// =================================================
 /**
  * makeTokens makes a subset of tokens from
  *  the list of tokens 0 thru the pEnd
  * 
  * @param pTokens
  * @param pEnd
  * @return List<Annotation>
 */
 // =================================================
private final List<Annotation> makeTokens(List<Annotation> pTokens, int pBegin, int pEnd) {
   ArrayList< Annotation> someTokens = null;
   
   if ( pTokens != null && pTokens.size() > 0 ) {
     someTokens = new ArrayList<Annotation>( pTokens.size() );
     for ( int i = pBegin; i < pEnd; i++) {
       if ( ((WordToken)pTokens.get(i)).getPunctuationOnly() )
         continue;
      
       else if ( !((WordToken)pTokens.get(i)).getSentenceBreak() )
         someTokens.add( pTokens.get(i));
     
     }
   }
   return someTokens;
} // end Method makeTokens() --------------------



// =================================================
/**
* isInOtherConcept 
* 
* @param pJCas
* @param statement
* @return boolean 
*/
//=================================================
protected boolean isInOtherConcept(JCas pJCas, Annotation pStatement) {

boolean returnVal = false;
Type conceptType = pJCas.getTypeSystem().getType("gov.va.vinci.model.Concept");

List<Annotation> otherConcepts = UIMAUtil.fuzzyFindAnnotationsBySpan(pJCas, conceptType, Concept.typeIndexID, pStatement.getBegin(), pStatement.getEnd(), true);

if ( otherConcepts != null && !otherConcepts.isEmpty()) 
     if ( otherConcepts.get(0).getClass().getSimpleName().contains("UnitOfMeasure")  || 
        otherConcepts.get(0).getClass().getSimpleName().contains("Number")    )
      returnVal = false;
    else 
      returnVal = true;
  
return returnVal;     
} // end Method isInOtherConcept() -----------------


//=================================================
/**
* isInContentHeading 
* 
* @param pJCas
* @param pStatement
* @return boolean
*/
//=================================================
protected boolean isInContentHeading(JCas pJCas, Annotation pStatement) {

boolean returnVal = false;

List<Annotation> conceptInHeading = UIMAUtil.fuzzyFindAnnotationsBySpan(pJCas, ContentHeading.typeIndexID, pStatement.getBegin(), pStatement.getEnd() );

if ( conceptInHeading != null && !conceptInHeading.isEmpty())
 returnVal = true;

return returnVal;

} // end Method isInContentHeading() 




//----------------------------------
/**
 * destroy
* 
 **/
// ----------------------------------
public void destroy() {
  this.performanceMeter.writeProfile( this.getClass().getSimpleName());
}


  //----------------------------------
  /**
   *  initialize    This is the standard uima way to pass parameters to an annotator.
   *                It is cumbersome.  It requires creating a config file with params
   *                in it, making it difficult to dynamically pass in parameters. 
   *                
   *                This method merges the uima way and keeping the ability to dynamically
   *                pass parameters into the class via - putting all parameters in a string
   *                array called "args" with each row containing a --key=value format.
   *                This way, arguments could be directly passed from command line,
   *                or read from a config file, or dynamically added to that string
   *                passed in. 
   *                
   *                It is important to adhere to the posix style "--" prefix and
   *                include a "=someValue" to fill in the value to the key. 
   * 
   * @param aContext
   * @throws ResourceInitializationException
   * 
   **/
  // ----------------------------------
  public void initialize(UimaContext aContext) throws ResourceInitializationException {
       
     
      String[] args = null;
      try {
        args                 = (String[]) aContext.getConfigParameterValue("args");  

        initialize(args);
        
      } catch (Exception e ) {
        String msg = "Issue in initializing class " + this.getClass().getName() + " " + e.toString() ;
        GLog.println(GLog.ERROR_LEVEL, msg);     // <------ use your own logging here
        throw new ResourceInitializationException();
      }
      
  
  } // end Method initialize() -------
  
  //----------------------------------
  /**
   * initialize initializes the class.  Parameters are passed in via a String
   *                array  with each row containing a --key=value format.
   *                
   *                It is important to adhere to the posix style "--" prefix and
   *                include a "=someValue" to fill in the value to the key. 
   * @param pArgs
   * @throws  ResourceInitializationException            
   * 
   **/
  // ----------------------------------
  public void initialize(String[] pArgs) throws ResourceInitializationException {
       
    
    String terminologyFilez = null;
    
    try {
    this.performanceMeter = new ProfilePerformanceMeter( pArgs, this.getClass().getSimpleName() );
    this.terminologies     = U.split(U.getOption(pArgs,  "--terminologies=", "SNOMEDCT_US:CPT:RXNORM"), ":" ); // LNC is not working 
    this.filterBySection    = Boolean.parseBoolean(U.getOption(pArgs,  "--filterBySection=", "true"));
    
    
    terminologyFilez = U.getOption(pArgs, "--terminologyFiles=", "");
    String[] terminologyFiles = U.split(terminologyFilez, ":");
    
    
    
    this.terminologyService = new  TermLookupLocalTermsImpl();
    this.terminologyService.init(pArgs, terminologyFiles);
    
    String knownAcronymsFile = "resources/com/ciitizen/framework/tokenizer/knownAcronyms.txt";
   
     this.tokenize = new TermUtils(knownAcronymsFile);
      
   //  this.wsdAnnotator = new WSDAnnotator( pArgs);
     
     // -------------------------
     // Set this to the annotation type being created in this class
     // -------------------------
     this.annotationType = "LexicalElement";
     
     // ------------------------
     // read in the section/type resource
     this.sectionInfo = new SectionMetaInfo( );
      
    } catch (Exception e) {
      e.printStackTrace();
      String msg = "Issue initizlizng Loretta " + e.toString();
      GLog.println(GLog.ERROR_LEVEL, this.getClass(), "initialize", msg);
      throw new ResourceInitializationException();
    }
  } // end Method initialize() -------
  
  

  // ---------------------------------------
  // Global Variables
  // ---------------------------------------
  protected int annotationCtr = 0;
  
  protected int                           annotationCounter  = 0;    // new Term Counter.
  protected TermLookupInterface           terminologyService = null;
  protected String[]                      terminologies = {"SNOMEDCT_US","SNOMEDCT"};
  protected String                        tree = null;
  private ProfilePerformanceMeter       performanceMeter   = null;
  private boolean                       processMe          = true;
  private TermUtils                     tokenize           = null;
  // private  WSDAnnotator                 wsdAnnotator       = null;
  protected String                      annotationType     = null;
  private SectionMetaInfo               sectionInfo        = null;
  protected boolean                     filterBySection    = true;
  
  
} // end Class LorettaAnnotator() ---------------

